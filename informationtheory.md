### 情報理論

#### 情報量

$$f(A)+f(B)=f(AB)$$
情報量の加法性
$f(x)$ は $x$ を知った時の情報量は次のように定義する。

確率が$p$で起こる事象が起こったことを知ったときの情報量
$$-\log_2{p}$$

起こるかどうかについて知りたい時が多いよねということで、*エントロピー*を$n$個の事象の確率を$p_i$とおいて、次のように定義する。
$$H(p_1,p_2,...,p_n)=-\sum_{i}{p_i\log{p_i}}$$

エントロピーの性質

* 非負
* 最大値は $H=\log{n}$ で, すべての事象が$p=1/n$で起こる時
