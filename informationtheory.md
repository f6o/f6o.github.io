### 情報理論

#### 情報量

$$f(A)+f(B)=f(AB)$$
情報量の加法性
$f(x)$ は $x$ を知った時の情報量

確率が$p$で起こる事象が起こったことを知ったときの情報量
$$-\log_2{p}$$

起こるかどうかについて知りたい時が多いよねということで、*エントロピー*を次のように定義する。
$$\sum_{i}{p\log{p_i}}$$
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTUwNDY4OTI5MCwtMzI3NjQwNjU3XX0=
-->